\section{Conclusion} \label{sec:conclusion}

We identified and resolved a fundamental limitation of neural posterior estimation: boundary pathology caused by sharp discontinuities in uniform proposal distributions, which is exacerbated in high-dimensional spaces and when parameter degeneracies extend posteriors toward prior boundaries. To address this issue, we proposed the \textit{Tailed-Uniform} proposal, which makes use of smooth, continuously differentiable Gaussian tails to smooth the abrupt boundaries. We validated the \textit{Tailed-Uniform} proposal on both synthetic and real-world inference problems, demonstrating substantial improvements over traditional uniform sampling.
Perhaps most critically, our ablation studies revealed that the boundary pathology is fundamental rather than a consequence of insufficient training data. Our method also exhibited counterintuitive scaling behavior in higher dimensions: despite allocating increasingly more samples to tail regions, \textit{Tailed-Uniform} maintains good performance at a reasonable number of dimensions. By extending the training distribution beyond the primary region of interest, our method provides neural networks with the support they require to learn posterior approximations near the boundaries, improving the quality of the posterior across the parameter space.
