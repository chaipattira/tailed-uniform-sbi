
\section{Toy Problem} \label{sec:toy}
\subsection{Gaussian Linear Task}
We formulate a toy problem using the Gaussian Linear task from the simulation-based inference benchmark (sbibm) \citep{lueckmann2021}. A Gaussian distribution is ubiquitous in the astrophysical enterprise, as it is---to borrow the jargon of information theory---the \textit{highest entropy} option \citep{cover2006}. One may say that it emerges almost inherently when many independent processes aggregate, per the Central Limit Theorem.

\textbf{Model} \;\;As a proof-of-concept, we consider a simple, linear, 2-dimensional Gaussian simulator $\mathcal{M}: \mathbb{R}^2 \to \mathbb{R}^2$, which is defined as
\begin{equation}
\mathcal{M}(\boldsymbol{\theta}) = \mathbf{A}\boldsymbol{\theta} = \mathbf{I}_2 \boldsymbol{\theta} = \boldsymbol{\theta}
\end{equation}
with a standard Gaussian prior $\mathcal{N}(\mathbf{0}, \mathbf{I}_2)$ centered in the bounded parameter space $\boldsymbol{\theta} \in [-1,1]^2$. 
We hence have a likelihood of the form
\begin{equation}
\mathcal P(\mathbf{x}\mid\boldsymbol{\theta}) = \mathcal{N}(\mathbf{x}; \boldsymbol{\theta}, \mathbf{I}_2) = \frac{1}{2\pi} \exp\left(-\frac{1}{2}\|\mathbf{x} - \boldsymbol{\theta}\|^2\right),
\end{equation}
which implies that both the likelihood and prior are Gaussian.

\textbf{Ground Truth} \;\; We can leverage the aforementioned conjugate Gaussian-Gaussian structure \citep{gelman2013} to obtain a closed-form analytical posterior $\mathcal{P}(\boldsymbol{\theta} \mid \mathbf{x}) = \mathcal{N}\left(\frac{\mathbf{x}}{2}, \frac{\mathbf{I}_2}{2}\right)$, where the posterior covariance $\frac{\mathbf{I}_2}{2}$ results from $(\mathbf{I}_2^{-1} + \mathbf{I}_2^{-1})^{-1} = \frac{\mathbf{I}_2}{2}$. By running Markov Chain Monte Carlo (MCMC) chains \citep{brooks2011}, we can generate \textit{reference posterior} samples from which to evaluate performance.

\textbf{Training}\:\: Our goal is to investigate two distinct strategies for generating the training data \citep{ho2024}:
\begin{enumerate}
    \item Baseline (\textit{Uniform} Proposal): Training data is generated by sampling parameters \textit{uniform}ly across the 2-dimensional hypercube $\tilde{\mathcal{P}}_{\text{\textit{Uniform}}} = \mathcal{U}([-1, 1]^2)$.
    \item Proposed (\textit{Tailed-Uniform} Proposal): Training data is generated using our hybrid distribution $\tilde{\mathcal{P}}_{\text{TailedUniform}}(\mathbf{a}=[-1,-1], \mathbf{b}=[1,1], \boldsymbol{\sigma}=[0.2, 0.2])$, where the tail width $\sigma_i = 0.1 \times (b_i - a_i)$ represents 10\% of each parameter range.
\end{enumerate}
For both proposals, we generate $N = 4000$ simulation pairs $\{(\boldsymbol{\theta}_i, \mathbf{x}_i)\}_{i=1}^N$ by: (1) sampling $\boldsymbol{\theta}_i$ from the respective proposal distribution, (2) running the forward simulator $\mathbf{x}_i = \mathcal{M}(\boldsymbol{\theta}_i) + \boldsymbol{\epsilon}_i$ with $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}_2)$
and (3) training identical neural posterior estimators (NPE) on each dataset. Note again that the assumed prior in both cases is a Gaussian distribution centered at $(0,0$). In what follows, we will associate \textit{Uniform} and \textit{Tailed-Uniform} to the NPE trained with these proposals (rather than the proposal itself).

\textbf{Model Architectures}\:\: We experiment with ensembles of neural density estimators comprising Masked Autoregressive Flows (MAF) \citep{papamakarios2017maf} and Masked Autoencoder for Distribution Estimation (MADE) \citep{germain2015} architectures, each featuring 16 hidden features and 5 coupling layers. Training uses a batch size of 64 and a learning rate of $5 \times 10^{-5}$. Our pipeline is built on the \texttt{LtU-ILI} (Learning the Universe—Implicit Likelihood Inference) framework \citep{ho2024}, which offers standardized data handling and ensemble utilities for simulation-based inference.
It is worth pointing out that there is nothing special about the above configuration. The Gaussian Linear task, with its analytically tractable posterior and low dimensionality, is simple enough for any garden-variety neural network to learn the target distribution.

\subsection{Validation}

Model validation aims to ascertain whether the approximate posteriors generated by our neural density estimators $q_w(\boldsymbol{\theta} \mid \mathbf{x}_0)$ will be ``good enough" when faced with real unlabeled observational data $\mathbf{x}_0$. This is no trivial matter, as we do not a priori know the learned weights in the hidden layers, and the internal workings of neural networks, qua oracles, are more or less invisible.

\textbf{The bias-variance tradeoff} \:\:One useful proxy for gauging the quality of our learned posterior is the degree to which it can concentrate probability mass around the true parameter $\boldsymbol{\theta}_\text{true}$. Practically worthless, for instance, will be a model that regurgitates the prior distribution, as it offers no additional insight whatsoever. After all, the goal of inference is to extract as much information as possible from the observed data $\mathbf x_0$ to narrow down the possible parameter values $\boldsymbol{\theta}$.
Yet, if one only optimizes for the training set, the resulting model might end up overfitting and fail to generalize when exposed to new data.

Achieving the sweet spot between bias and variance \citep{gelman1992} is thus the characteristic of a ``good" model, as we want a model that is versatile enough to extract genuine information from observations (low bias) while remaining generalizable across different realizations of the data (low variance).
\begin{table}[ht]
\centering
\caption{Posterior Performance for $\boldsymbol{\theta}_{\text{true}} = (0.6, 0.6)$.}
\label{tab:onetable}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \multicolumn{2}{c}{\textbf{Parameter Estimates}} & \textbf{C2ST} \\
\cmidrule(lr){2-3}
& $\theta_1$ & $\theta_2$ & \\
\midrule
Reference & $0.446 \pm 0.300$ & $0.641 \pm 0.308$ & 0.500 \\
\midrule
\textit{TailedUniform} & $0.339 \pm 0.547$ & $0.708 \pm 0.286$ & \textbf{0.458} \\
\midrule
\textit{Uniform} & $0.259 \pm 0.446$ & $0.588 \pm 0.272$ & 0.409 \\
\midrule
\textbf{Improvement} & \textbf{+31\%} & \textbf{+10\%} & \textbf{+12\%} \\
\bottomrule
\end{tabular}
\end{table}
\subsubsection{Tier One: Single-Point Analysis}

As a first pass, we select test points $\boldsymbol{\theta}_{\text{true}}$ near the boundaries of parameter space, namely $\boldsymbol{\theta}_{\text{true}} = (0.6, 0.6)$ (i.e., $3\sigma$ away from the center of the prior) and draw $M = 1000$ independent and identically distributed samples from the learned posterior. The goal is to assess their constraining power by investigating the distribution and shape around the true value.

\begin{figure}
    \centering
    \includegraphics[width=1.02\linewidth]{corner.pdf}
    \caption{Corner plots comparing posterior estimation performance for the boundary test case. The \textit{Tailed-Uniform} (green) demonstrates superior boundary behavior compared to the \textit{Uniform} (blue), closely matching the MCMC reference (yellow). The red dashed line indicates the true parameter value.}
    \vspace{1mm}
    \label{fig:onetest}
\end{figure}

\textbf{Quantitative Metrics} \;\; We also use the Classifier two-sample tests (abbreviated C2ST) \citep{lopez-paz2016}, which is recognized as one of the best interpretable metrics for comparing two distributions. It trains a logistic regression classifier to differentiate between samples from two distributions: a score of 0.5 indicates identical distributions (desirable), whereas diverging scores indicate easily distinguishable distributions (poor performance).

As anticipated in Figure~\ref{fig:onetest}, the \textit{Tailed-Uniform} (orange) keeps a tight concentration around the true parameter values, while the \textit{Uniform} (blue) leaks into unusable parameter regions.
Table \ref{tab:onetable} further corroborates \textit{Tailed-Uniform}’s superior performance
across all metrics over the \textit{Uniform} baseline.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{heatmap.pdf}
        \caption{Spatial distribution of C2ST performance across the parameter space, with blue regions indicating poor distributional matching (C2ST $\ll 0.5$) and red/orange regions indicating good performance (C2ST $\approx 0.5$) \textbf{Top}: \textit{Uniform} versus analytical reference, showing systematic boundary degradation with glaring blue regions near parameter space edges. \textbf{Bottom}: \textit{TailedUniform} versus reference, demonstrating consistent performance across the entire parameter space.}
    \label{fig:c2st_heatmaps}
\end{figure}

\subsubsection{Tier Two: Spatial Performance Analysis}

Next we need a way to assess \textit{Tailed-Uniform}'s performance across the entire parameter space. 
And so, we discretize a uniform rectangular grid with $n = 20$ test locations along each dimension of the parameter space $[-1, 1]^2$, yielding a total of 400 evaluation points. We want to exhaustively sample areas of varying distances, ranging from the center of the priors, where traditional techniques work well, to points near the boundaries.
As before, we generate observations for each test point, draw $M = 1000$ posterior samples from all three methods, and calculate pairwise C2ST scores at each test location. 

Figure~\ref{fig:c2st_heatmaps} shows how \textit{Uniform}'s performance deteriorates as it drifts away from the central regions, as evidenced by the proliferation of blue patches near the boundary. In contrast, our \textit{Tailed-Uniform} maintains more red and orange coloration (with C2ST scores around $0.44$--$0.49$) across the parameter space.

Furthermore, we observe that the resulting posterior is spherically symmetric (with equal variance in all directions), allowing us to visualize the improvements by binning the test points as a radial function from the center of our parameter space, as seen in Figure~\ref{fig:c2st-curve}.
It is, if nothing else, evident that \textit{Tailed-Uniform} exhibits robustness and mitigates the undesired boundary pathology that characterizes the traditional sampling technique.
\begin{figure}
    \centering
    \vspace{5pt}%
    \includegraphics[width=1\linewidth]{c2st-radius.pdf}
    \caption{C2ST performance degradation as a function of distance from parameter space center. The blue curve reveals systematic deterioration of the Uniform NPE method near boundaries, while the green curve demonstrates that TailedUniform NPE maintains consistent performance across all radii. The gray curve quantifies the increasing divergence between methods, with boundary regions showing substantial differences in posterior approximation quality. Values closer to 0.5 indicate better distributional matching.}
    \label{fig:c2st-curve}
    \vspace{5pt}%
\end{figure}
\subsection{Discussion} \label{subsec:bias}
Our subsequent evaluations aim to answer the following operational questions:
\begin{enumerate}
    \item How large should the tails be?
    \item Could we mitigate the boundary pathology by throwing more data at the NPE?
    \item How does \textit{Tailed-Uniform} perform in higher dimensions?
\end{enumerate}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{c2st-vs-distance-by-sigmas.pdf}
    \caption{C2ST performance versus distance from center, stratified by tail width $\sigma$. The \textit{Uniform} baseline (blue) exhibits systematic boundary degradation. The \textit{Tailed-Uniform}'s with varying tail widths (shown in green; lighter shades for smaller $\sigma$ values) exhibit robustness across a broad range of tail widths. Notably, performance becomes quite good
for $\sigma \geq 0.2$}
    \label{fig:ablation_sigma}
\end{figure}

\subsubsection{Ablation on the tail widths}
The tail width $\sigma$ represents a bias-variance tradeoff: narrower tails
recover the boundary pathologies, while wider tails allocate more simulations
outside the primary region of interest. To assess the importance of the tails,
we train one \textit{Uniform} baseline and several \textit{Tailed-Uniform}
models, each with different $\sigma$ values as fractions of prior width:
$\alpha \in \{0.01, 0.05, 0.1, 0.2, 0.4\}$.

Figure~\ref{fig:ablation_sigma} reveals that performance improves monotonically
with tail width. At the prior boundary (r=1.0), C2ST scores improve from 0.331
($\sigma=0.01$, equivalent to Uniform) to 0.465 ($\sigma=0.4$, near-ideal),
demonstrating the value of wider tails.
The reassuring robustness indicates that even moderate tail samples suffice for learning smooth density transitions, which will prove insightful in higher dimensions. It also implies that practitioners do not need to hyperparameter tune $\sigma$, as any reasonable choice within this range will suffice.
At $\sigma = 0.01$, \textit{Tailed-Uniform} degenerates as anticipated toward \textit{Uniform} behavior with characteristic boundary degradation, confirming that the tails themselves (not some arbitrary artifacts) are responsible for the observed improvements.

\subsubsection{Ablation on the number of simulations}
If insufficient data in the near-boundary regions causes degradation, just increasing the training set size could very well solve the problem. After all, neural networks are universal function approximators, so given enough training data, they should be able to learn complex mappings, including sharp transitions at parameter space boundaries.
To interrogate this hypothesis, we test an ablation over the training set size, varying $N \in \{2000, 4000, 8000, 16000\}$ while holding all other hyperparameters fixed.

Empirically, we find in Figure~\ref{fig:ablation_nsims} that increasing the number of simulations barely helps with the NPE's performance, suggesting that the boundary degradation stems not from absence of
data, but from an inbuilt discontinuous mapping that cannot be learned
from samples within the prior alone. Since no amount of supplementary data can smooth a non-smooth function, the hard truncation at prior boundaries poses a fundamental learning barrier. In contrast, we see that every single \textit{Tailed-Uniform} ($N = 2000$) outperforms even the largest \textit{Uniform} ($N = 16000$) with 8 times more training data, demonstrating the impact of different proposals. One can achieve better posterior quality at a fraction of the computational cost just by sampling in a judicious manner.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{c2st-vs-distance-by-nsims.pdf}
    \caption{C2ST performance versus distance from parameter space center, stratified by training set size $N$ (circles: 2000, squares: 4000, triangles: 8000, diamonds: 16000). Increasing $N$ provides minimal benefit for both proposals.}
    \label{fig:ablation_nsims}
\end{figure}

\subsubsection{Scaling to higher dimensions}

While we have shown \textit{Tailed-Uniform} to be effective in two dimensions, a natural question arises: does this benefit transfer to higher-dimensional parameter spaces, which are necessary for the majority of astrophysical inference problems? This is indeed a critical concern. By the curse of dimensionality, high-dimensional spaces become ever more sparse, as the volume of a $d$-dimensional hypercube grows exponentially while the number of samples remains fixed. Corners, edges, and boundary regions come to dominate the landscape.

Having proffered stakes, let's examine the probability of obtaining samples near boundaries.
For a $d$-dimensional hypercube $[0,1]^d$, we define the interior region as the set of points at least a distance $\varepsilon$ away from any boundary, which occupies a fraction
$P_{\text{int}}^{\text{Uniform}}(d, \varepsilon) = (1 - 2\varepsilon)^d$
of the total volume.
Complementarily, the probability that a uniformly sampled point lies in the boundary shell (within $\varepsilon$ of any face) grows exponentially as $P_{\text{bdry}}^{\text{Uniform}}(d, \varepsilon) = 1 - (1 - 2\varepsilon)^d$, which approaches unity at high dimensions.

\begin{table}[ht]
\centering
\caption{Sampling budget across regions for \textit{Uniform} and \textit{Tailed-Uniform} proposals ($\varepsilon = 0.05$, $\alpha = 0.1$).}
\label{tab:dim_scaling}
\begin{tabular}{lcccccc}
\toprule
& \multicolumn{3}{c}{\textbf{Uniform}} & \multicolumn{3}{c}{\textbf{Tailed-Uniform}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
$d$ & $P_{\text{int}}$ & $P_{\text{bdry}}$ & $P_{\text{tail}}$ & $P_{\text{int}}$ & $P_{\text{bdry}}$ & $P_{\text{tail}}$ \\
\midrule
2 & 0.810 & 0.190 & 0.000 & 0.617 & 0.145 & 0.238 \\
4 & 0.656 & 0.344 & 0.000 & 0.381 & 0.200 & 0.419 \\
8 & 0.430 & 0.570 & 0.000 & 0.145 & 0.192 & 0.663 \\
16 & 0.185 & 0.815 & 0.000 & 0.021 & 0.115 & 0.864 \\
\bottomrule
\end{tabular}
\end{table}
\textbf{The Tail Allocation Budget} \;\; For \textit{Tailed-Uniform}, samples extend beyond the hypercube boundaries into tail regions.
With independent marginals, the probability that a sample lands inside the $d$-dimensional hypercube is the product
\begin{equation}
P_{\text{cube}}^{\text{TailedUniform}}(d) = \prod_{i=1}^{d} B_i = B^d = \left(\frac{1}{1 + \alpha\sqrt{2\pi}}\right)^d,
\end{equation}
assuming identical $\alpha$ across dimensions. This means the probability of sampling in the tail regions grows as
$P_{\text{tail}}^{\text{TailedUniform}}(d) = 1 - B^d$. By the same token, the probability of landing in the interior becomes $P_{\text{int}}^{\text{TailedUniform}}(d, \varepsilon) = B^d \cdot (1 - 2\varepsilon)^d$,
while the boundary shell captures $P_{\text{bdry}}^{\text{TailedUniform}}(d, \varepsilon) = B^d \left[1 - (1 - 2\varepsilon)^d\right]$. Table~\ref{tab:dim_scaling} summarizes these quantities for representative dimensions.


At first glance, it might seem that by bleeding samples into the tails, too many samples will wind up outside the region of interest, thus diminishing the overall performance of \textit{Tailed-Uniform}. To refute this suspicion, we evaluate NPE trained with \textit{Uniform} and \textit{Tailed-Uniform} proposals on a Gaussian linear inference task across $d \in \{4, 8, 16\}$ dimensions. 

\textbf{Boundary Dominance in Higher Dimensions:} \;\; Figure \ref{fig:dim} shows that while both methods degrade with increasing dimensionality---an expected consequence of the aforementioned ``curse of dimensionality"---\textit{Tailed-Uniform} consistently outperforms \textit{Uniform}, with the performance gap widening at higher dimensions.
This somewhat paradoxical finding suggests that the ratio of information gained from exterior support to cost of interior sample dilution remains favorable or even improves with dimension. Extra samples from \textit{Tailed-Uniform} flow into the tail regions, surrounding the boundaries and offering regularization.
Indeed, at $d=16$, boundaries are so dominant that the marginal value of additional interior samples is low, as the network has ample interior coverage even when 99.99\% of \textit{Tailed-Uniform} samples lie outside the region of interest. Meanwhile, the marginal value of tail samples is high, as they are the only source of information near the boundary, which explains why \textit{Tailed-Uniform} still achieves slightly better C2ST scores than \textit{Uniform} at the boundary and in the extrapolation regime (2$\sigma$ beyond the prior boundary).
We acknowledge that at very high dimensions, dilution effects will eventually dominate, but these regimes are beyond the scope of most practical simulation-based inference tasks.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/c2st-vs-distance-by-dimension.pdf}
    \caption{C2ST performance versus distance from parameter space center in different representative dimensions. \textit{Tailed-Uniform} maintains superior performance with the advantage 
growing at higher dimensions, particularly near boundaries ($r \geq 0.75$) and 
in the extrapolation regime.}
    \label{fig:dim}
\end{figure}